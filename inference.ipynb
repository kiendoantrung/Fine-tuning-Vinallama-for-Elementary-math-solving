{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"execution":{"iopub.execute_input":"2024-02-01T08:49:40.729810Z","iopub.status.busy":"2024-02-01T08:49:40.728767Z","iopub.status.idle":"2024-02-01T08:52:16.766426Z","shell.execute_reply":"2024-02-01T08:52:16.765074Z","shell.execute_reply.started":"2024-02-01T08:49:40.729773Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["! pip install -q -U bitsandbytes\n","! pip install -q -U datasets\n","! pip install -q -U git+https://github.com/huggingface/transformers.git\n","! pip install -q -U git+https://github.com/huggingface/peft.git\n","! pip install -q -U git+https://github.com/huggingface/accelerate.git\n","! pip install -q -U loralib\n","! pip install -q -U einops"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-02-01T08:52:16.769282Z","iopub.status.busy":"2024-02-01T08:52:16.768888Z","iopub.status.idle":"2024-02-01T08:52:22.983514Z","shell.execute_reply":"2024-02-01T08:52:22.982758Z","shell.execute_reply.started":"2024-02-01T08:52:16.769247Z"},"trusted":true},"outputs":[],"source":["import json\n","import os\n","import bitsandbytes as bnb\n","import torch\n","import torch.nn as nn\n","import transformers\n","from pprint import pprint\n","from datasets import load_dataset\n","from huggingface_hub import notebook_login\n","from peft import (\n","  LoraConfig,\n","  PeftConfig,\n","  PeftModel,\n","  get_peft_model,\n","  prepare_model_for_kbit_training\n",")\n","from transformers import (\n","  AutoConfig,\n","  AutoModelForCausalLM,\n","  AutoTokenizer,\n","  BitsAndBytesConfig\n",")\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-02-01T08:52:22.985230Z","iopub.status.busy":"2024-02-01T08:52:22.984680Z","iopub.status.idle":"2024-02-01T08:54:06.848420Z","shell.execute_reply":"2024-02-01T08:54:06.847634Z","shell.execute_reply.started":"2024-02-01T08:52:22.985196Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e05e5adf42f64294863b607895795871","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/681 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"83f996b32c2b458485053efdd803d399","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9e57e3b7212d478a901a08b8c85f8f49","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d0277eaaac1243609c956f52f0b7b407","version_major":2,"version_minor":0},"text/plain":["pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.91G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"00223f38b0694868a2356730769e3ea6","version_major":2,"version_minor":0},"text/plain":["pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.80G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"857ccdc4866c406ebadf1c0b0748b8be","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a8fa193e73dc4b7cae55007861e04873","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f470383d5ba64098ab768cb5c0b12f79","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.22k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"862ca89f83ee49f0b959d7ad12869b87","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/2.67M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1fd69723ef6944f293133a5bb6052877","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/557 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["MODEL_NAME = \"vilm/vinallama-7b-chat\"\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit = True,\n","    bnb_4bit_use_double_quant = True,\n","    bnb_4bit_quant_type = \"nf4\",\n","    bnb_4bit_compute_dtype = torch.bfloat16)\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    MODEL_NAME,\n","    device_map = \"auto\",\n","    trust_remote_code = True,\n","    quantization_config = bnb_config\n",")\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","tokenizer.pad_token = tokenizer.eos_token\n","model.gradient_checkpointing_enable()\n","model = prepare_model_for_kbit_training(model)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-02-01T08:54:51.553461Z","iopub.status.busy":"2024-02-01T08:54:51.552684Z","iopub.status.idle":"2024-02-01T08:54:57.038180Z","shell.execute_reply":"2024-02-01T08:54:57.037352Z","shell.execute_reply.started":"2024-02-01T08:54:51.553415Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fda67f8c945244a3ae936a5465019861","version_major":2,"version_minor":0},"text/plain":["adapter_config.json:   0%|          | 0.00/675 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"785d4ffa102b4d85b6d8c2836b06c55d","version_major":2,"version_minor":0},"text/plain":["adapter_model.safetensors:   0%|          | 0.00/160M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["config = PeftConfig.from_pretrained(\"kiendt/vinallama-math-7b\")\n","model = PeftModel.from_pretrained(model, \"kiendt/vinallama-math-7b\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-02-01T08:55:00.404728Z","iopub.status.busy":"2024-02-01T08:55:00.404047Z","iopub.status.idle":"2024-02-01T08:55:00.409711Z","shell.execute_reply":"2024-02-01T08:55:00.408745Z","shell.execute_reply.started":"2024-02-01T08:55:00.404694Z"},"trusted":true},"outputs":[],"source":["generation_config = model.generation_config\n","generation_config.max_new_tokens = 200\n","generation_config.temperature = 0.7\n","generation_config.top_p = 0.7\n","generation_config.num_return_sequences = 1\n","generation_config.pad_token_id = tokenizer.eos_token_id\n","generation_config.eos_token_id = tokenizer.eos_token_id"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-02-01T08:55:02.181393Z","iopub.status.busy":"2024-02-01T08:55:02.180313Z","iopub.status.idle":"2024-02-01T08:55:36.022157Z","shell.execute_reply":"2024-02-01T08:55:36.021171Z","shell.execute_reply.started":"2024-02-01T08:55:02.181355Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]},{"name":"stdout","output_type":"stream","text":["<| im_start|>system\n","Bạn là một chuyên gia về toán. Bạn sẽ nhận câu hỏi trắc nghiệm kèm theo các lựa chọn, hãy giải step by step nếu có và chọn phương án đúng.\n","<|im_start|> user\n","### Câu hỏi:\n","Mua 15 cái bút hết 45 000 đồng. Vậy mua 40 cái bút hết số tiền là:\n","### Các lựa chọn:\n","A. 120 000 đồng\n","B. 240 000 đồng\n","C. 675 000 đồng\n","D. 15 000 đồng\n","### Câu trả lời:\n","\n","<|im_start|> assistant\n","Đáp án A Mua 15 cái bút hết 45 000 đồng. Vậy mua 40 cái bút hết số tiền là: 45 000 – 15 000 = 120 000 đồng. Đáp số: 120 000 đồng. Đáp án A. Đáp án đúng là: 120 000 đồng. Vậy ta chọn đáp án: A. 120 000 đồng. Đáp án cần chọn là: A. Đáp án cần chọn là: A. 120 000 đồng. Đáp án cần chọn là: A. 120 000 đồng. Đáp án cần chọn là: A. 120 000 đồng. Đáp án cần chọn là: A. 120 000 đồng. Đáp án cần chọn là: A. 120 000 đồng. Đáp án cần chọn là: A. 120 000 đồng. Đáp án cần chọn là: A. 120 000 đồng. Đáp án cần chọn là: A. 120 000 đô\n"]}],"source":["import torch\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","prompt = \"\"\"\n","<| im_start|>system\n","Bạn là một chuyên gia về toán. Bạn sẽ nhận câu hỏi trắc nghiệm kèm theo các lựa chọn, hãy giải step by step nếu có và chọn phương án đúng.\n","<|im_start|>user\n","### Câu hỏi:\n","Mua 15 cái bút hết 45 000 đồng. Vậy mua 40 cái bút hết số tiền là:\n","### Các lựa chọn:\n","A. 120 000 đồng\n","B. 240 000 đồng\n","C. 675 000 đồng\n","D. 15 000 đồng\n","### Câu trả lời:\n","\n","<|im_start|>assistant\n","\"\"\".strip()\n","\n","encoding = tokenizer(prompt, return_tensors = \"pt\").to(device)\n","with torch.inference_mode() :\n","  outputs = model.generate(input_ids = encoding.input_ids,\n","                              attention_mask = encoding.attention_mask,\n","                              generation_config = generation_config)\n","print(tokenizer.decode(outputs[0], skip_special_tokens = True))"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-02-01T09:13:13.854329Z","iopub.status.busy":"2024-02-01T09:13:13.853933Z","iopub.status.idle":"2024-02-01T09:13:36.621064Z","shell.execute_reply":"2024-02-01T09:13:36.620110Z","shell.execute_reply.started":"2024-02-01T09:13:13.854298Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<| im_start|>system\n","Bạn là một chuyên gia về toán. Bạn sẽ nhận câu hỏi trắc nghiệm kèm theo các lựa chọn, hãy giải step by step nếu có và chọn phương án đúng.\n","<|im_start|> user\n","### Câu hỏi:\n","Nhân viên y tế mất khoảng 5 phút để tiêm xong vắc-xin cho một người. Bố xếp hàng đợi tiêm lúc 9 giờ 20 phút. Phía trước bố còn 6 người nữ. Vậy bố sẽ được tiêm vắc-xin xong lúc:\n","### Các lựa chọn:\n","A. 9 giờ 50 phút\n","B. 10 giờ 5 phút\n","C. 10 giờ kém 5 phút\n","D. 9 giờ 25 phút\n","### Câu trả lời:\n","\n","<|im_start|> assistant\n","Đáp án C Vậy bố sẽ được tiêm vắc-xin xong lúc: 9 giờ kém 5 phút. Đáp số: 9 giờ kém 5 phút. Đáp án C. \n"," Số người cần tiêm xong là: 6 + 1 = 7 (người) \n"," Vậy bố cần tiêm xong vò̉ 6 người trong 5 giờ (vì bố xếp hàng đợi tiêm lúc 9 giờ 20 mìn). \n"," Vậy bố cần tiêm xong vò̉ 6 người trong 5 giờ là: 5 : 5 = 1 giờ (vì 5 giờ = 1 giờ). \n"," Vậy bố sẽ được tiêm xong vò̉ 6 người trong 1 giờ 5 phút. \n"," Vậy bố sẽ được tiêm xong vò̉ 6 người trong 5\n"]}],"source":["prompt = \"\"\"\n","<| im_start|>system\n","Bạn là một chuyên gia về toán. Bạn sẽ nhận câu hỏi trắc nghiệm kèm theo các lựa chọn, hãy giải step by step nếu có và chọn phương án đúng.\n","<|im_start|>user\n","### Câu hỏi:\n","Nhân viên y tế mất khoảng 5 phút để tiêm xong vắc-xin cho một người. Bố xếp hàng đợi tiêm lúc 9 giờ 20 phút. Phía trước bố còn 6 người nữ. Vậy bố sẽ được tiêm vắc-xin xong lúc:\n","### Các lựa chọn:\n","A. 9 giờ 50 phút\n","B. 10 giờ 5 phút\n","C. 10 giờ kém 5 phút\n","D. 9 giờ 25 phút\n","### Câu trả lời:\n","\n","<|im_start|>assistant\n","\"\"\".strip()\n","\n","encoding = tokenizer(prompt, return_tensors = \"pt\").to(device)\n","with torch.inference_mode() :\n","  outputs = model.generate(input_ids = encoding.input_ids,\n","                              attention_mask = encoding.attention_mask,\n","                              generation_config = generation_config)\n","print(tokenizer.decode(outputs[0], skip_special_tokens = True))"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-02-01T09:47:08.231163Z","iopub.status.busy":"2024-02-01T09:47:08.230385Z","iopub.status.idle":"2024-02-01T09:47:31.032211Z","shell.execute_reply":"2024-02-01T09:47:31.031199Z","shell.execute_reply.started":"2024-02-01T09:47:08.231128Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<| im_start|>system\n","Bạn là một chuyên gia về toán. Bạn sẽ nhận câu hỏi trắc nghiệm kèm theo các lựa chọn, hãy giải step by step nếu có và chọn phương án đúng.\n","<|im_start|> user\n","### Câu hỏi:\n","Ngày thứ nhất chú Tài thu mua được 1 350 kg phế liệu, ngày thứ hai thu mua được nhiều hơn ngày thứ nhất 580 kg phế liệu. Vậy sau hai ngày chú Tài thu mua được số ki-lô-gam phế liệu là:\n","### Các lựa chọn:\n","A. 1 930 kg\n","B. 3 280 kg\n","C. 1 830 kg\n","D. 2 280 kg\n","### Câu trả lời:\n","\n","<|im_start|> assistant\n","Đáp án B Chú Tài thu mua được số ki-lô-gam phế liệu là: 1 350 + 580 = 2 280 (kg) Ngày thứ hai thu mua được số ki-lô-gam phế liệu là: 2 280 – 1 350 = 880 (kg) Ngày thứ ba thu mua được số ki-lô-gam phế liệu là: 880 + 580 = 3 280 (kg) Chú Tài thu mua được số ki-lô-gam phế liệu là: 1 350 + 580 + 880 = 2 280 (kg) Đáp số: 2 280 kg. Đáp án cần chọn là: B.  . Đáp án cần chọn là: B.  . Đáp án cần chọn là: B.  . Đáp án cần chọn là: B.  . Đáp án cần chọn là: B.  . Đáp án cần chọn là: B.  . Đáp án cần chọn là:\n"]}],"source":["prompt = \"\"\"\n","<| im_start|>system\n","Bạn là một chuyên gia về toán. Bạn sẽ nhận câu hỏi trắc nghiệm kèm theo các lựa chọn, hãy giải step by step nếu có và chọn phương án đúng.\n","<|im_start|>user\n","### Câu hỏi:\n","Ngày thứ nhất chú Tài thu mua được 1 350 kg phế liệu, ngày thứ hai thu mua được nhiều hơn ngày thứ nhất 580 kg phế liệu. Vậy sau hai ngày chú Tài thu mua được số ki-lô-gam phế liệu là:\n","### Các lựa chọn:\n","A. 1 930 kg\n","B. 3 280 kg\n","C. 1 830 kg\n","D. 2 280 kg\n","### Câu trả lời:\n","\n","<|im_start|>assistant\n","\"\"\".strip()\n","\n","encoding = tokenizer(prompt, return_tensors = \"pt\").to(device)\n","with torch.inference_mode():\n","  outputs = model.generate(input_ids = encoding.input_ids,\n","                              attention_mask = encoding.attention_mask,\n","                              generation_config = generation_config)\n","print(tokenizer.decode(outputs[0], skip_special_tokens = True))"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30636,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
